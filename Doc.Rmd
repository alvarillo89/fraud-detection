---
title: "Sistemas Inteligentes para la Gestión en la Empresa"
subtitle: "Práctica 1: Preprocesamiento de datos y clasificación binaria"
author: "Álvaro Fernández García"
date: "Curso 2019-2020"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: simplex
    highlight: tango
    number_sections: true
    df_print: paged
---

---

Antes que nada, para el desarrollo de la práctica se utilizarán los siguientes paquetes de R:

- `tidyverse`: para el manejo de los dataframes.
- `funModeling` y `ggplot2`: para el análisis exploratorio de los datos.
- `VIM`: para la imputación de variables perdidas.
- `caret`: para entrenar y validar modelos de clasificación.
- `pROC`: para calcular la curva ROC en la validación de los modelos.
- `earth`: para realizar la selección de características importantes.
- `DMwR`: para el método SMOTE.

```{r results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(VIM)
library(funModeling)
library(ggplot2)
library(caret)
library(pROC)
library(earth)
library(DMwR)
```

# Introducción

El pricipal objetivo de esta práctica es la resolución de un problema de preprocesamiento y aprendizaje automático, al mismo tiempo que se estudia la importancia de realizar un preprocesamiento apropiado. Para ello, se utilizará el conjunto de datos [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/data) en el que se pretende predecir la probabilidad de que una determinada transacción online sea o no fraudulenta.

Los datos están divididos en dos archivos `transaction` e `identity` (los cuales pueden unirse mediante la columna `TransactionID`). A continuación se describe el significado de cada una de las columnas de ambos datasets:

- `isFraud`: variable objetivo: 1 es fraude, 0 no lo es.
- `TransactionDT`: timedelta a partir de una fecha-hora de referencia dada (no es una marca de tiempo).
- `TransactionAMT`: importe de pago de la transacción en USD.
- `ProductCD`: código de producto, el producto para cada transacción.
- `card1 - card6`: información de la tarjeta de pago, como el tipo de tarjeta, la categoría de la tarjeta, el banco emisor, el país, etc.
- `addr`: dirección
- `dist`: distancia
- `P_ y R_ emaildomain`: dominio de correo electrónico del comprador y del destinatario
- `C1-C14`: contadores, como cuántas direcciones se encuentran asociadas a la tarjeta de pago, etc. El significado real está enmascarado.
- `D1-D15`: tiempo de espera, como por ejemplo, los días transcurridos entre la transacción anterior, etc.
- `M1-M9`: coincidencias, en los nombres, en la tarjeta, la dirección, etc.
- `Vxxx`: Vesta extrajo características ricas, incluyendo la clasificación, el conteo y otras relaciones de entidades.
- Las variables del dataset `identity` denotan: información de conexión a la red (IP, ISP, Proxy, etc.) y firma digital (UA/navegador/os/versión, etc.) asociada a las transacciones.

Todas las variables son numéricas, a excepción de las siguientes que son categóricas:

- **Transaction:**
  - ProductCD
  - card1 - card6
  - addr1, addr2
  - P_emaildomain
  - R_emaildomain
  - M1 - M9

- **Identity:**
  - DeviceType
  - DeviceInfo
  - id_12 - id_36
  
Se realizarán un total de **3 experimentos**, cada uno de ellos, con un preprocesamiento diferente.

# Modelos empleados

En este apartado se describen aquellos algoritmos de clasificación que se aplicarán para intentar predecir si una transacción bancaria es o no fraudulenta. Para que la comparación entre los preprocesamientos realizados sea justa, los modelos empleados en todas las pruebas son los mismos. También se mantienen los mismos hiperparámetros, los cuales han sido fijados con el conjunto datos preprocesado del [experimento 1](#experimento-1): 

1. `Random Forest`: se trata de un metaestimador que ajusta un número determinado de árboles de decisión a varias submuestras. Se ha escogido este modelo para este problema simplemente por experiencia. En el pasado he trabajado en varios problemas de clasificación sobre conjuntos de datos muy similares a los que se obtienen tras preprocesar el _IEEE Fraud Detection_ (aproximadamente 40.000 datos, decenas de columnas y datos numéricos y categóricos). En esos problemas, Random Forest fue el método que mejores resultados obtuvo. En general, los _ensembles_ de clasificadores son bastante robustos y Random Forest es el ejemplo clásico de ensemble. Los hiperparámetros fijados han sido dos: el número de árboles (600) y el número de características a elegir en cada muestra (10). Para escogerlos, se ha hecho validación cruzada de 5 folds sobre una submuestra del conjunto de entrenamiento, con 5.000 ejemplos aleatorios de cada clase (para hacer su elección mucho más eficiente).
2. `Extreme Gradient Boosting`: se trata de una implementación de `Gradient Boosting Decision Trees` diseñada teniendo en cuenta la velocidad y el rendimiento. Gradient Boosting es un enfoque en el que se añaden nuevos modelos (boosting) que predicen los residuos o errores de modelos anteriores y luego se suman para hacer la predicción final. Se llama Gradient Boosting porque utiliza el algoritmo del gradiente descendente para minimizar la pérdida al agregar nuevos modelos (que en este caso son árboles de decisión). Se ha escogido este modelo por su eficiencia y también porque está ampliamente utilizado en las competiciones de Kaggle con resultados excelentes, además de que puede manejar sin problemas variables categóricas e incluso valores perdidos. Su principal inconveniente es que presenta una gran cantidad de hiperparámetros, por lo que para escoger sus valores óptimos, se ha dejado que caret los elija de forma automática de entre los valores por defecto, utilizando un subjunto de 1.000 ejemplos aleatorios de cada clase. Consulte `scripts/train_xgb.R` para más información.

**Nota:** antes de investigar sobre el Extreme Gradient Boosting, se apostó por utilizar una `Regresión Logística` con penalización L2. Siempre que se hace frente a un nuevo problema de aprendizaje automático, hay que empezar por la clase de modelos más simples, esto es, los lineales. El motivo por el que finalmente se descartó la Regresión Logística es porque este modelo es exclusivo para datos numéricos, y realizar el `one hot encoding` de variables categóricas con más de 70 valores únicos (incluso agrupando categorías poco frecuentes bajo una misma etiqueta) introducía demasiadas columnas en el dataset con varianza cero (por lo que los datos no podían normalizarse). En el script `scripts/train_lr.R` se encuentra la prueba que se realizó para el [experimento 1](#experimento-1), en la cual solo se obtuvo alrededor de un 60% de Accuracy en train.

# Análisis exploratorio

Antes de empezar con los distintos experimentos debemos extraer información sobre los datos. Esto lo conseguimos gracias al *análisis exploratorio*. La información que se obtenga en este proceso, será de utilidad para definir el preprocesamiento que se realizará en el futuro.

Comenzaremos analizando en primer lugar el dataset _transaction_. Cargamos el CSV, y con la función *df_status* del paquete _funModeling_ extraemos un resumen del dataset. En él se obtiene información como el procentaje de valores perdidos en cada columna, el porcentaje de ceros, el tipo de las columnas, el número de valores únicos, etcétera. En definitiva, proporciona información muy relevante para comprender mejor los datos.

```{r, message=FALSE}
data.raw <- read_csv('ieee-fraud-detection/train_transaction.csv')
as.data.frame(df_status(data.raw, F))
```

Puede verse que muchas de las columnas tienen un alto porcentaje de valores perdidos (89%, 86%, e incluso 93%) y lo mismo sucede con los 0. Esto será algo a lo que nos trendremos que enfrentar en el preprocesamiento. También llama la atención la presencia de varibales categóricas con demasiados valores únicos, como por ejemplo `card1`, con 13.553 valores diferentes. O muchas columnas categóricas (como `M7`) que solo tienen como valores 0 o 1 (por lo que son interpretadas como lógicas).
  
A continuación, mostramos la dimensión del dataset:

```{r}
dim(data.raw)
```

Está formado por casi 600.000 transacciones, con 394 variables (y estas no son todas las disponibles). Esta alta dimensionalidad puede suponer un grave problema a la hora de preprocesar y entrenar los modelos, por lo que habrá que aplicar técnicas para intentar reducirla.

También es de vital importancia conocer el balanceo de las clases, es decir, cuantos ejemplos hay de la clase "Es fraude" y cuantos de la clase "No es fraude":

```{r}
table(data.raw$isFraud)
prop.table(table(data.raw$isFraud))
```

Podemos ver que el desbalanceo es bastante acusado: el 96.5% de los datos pertenecen a la clase 0 ("No es fraude") mientras que solo el 3.5% pertenecen a la clase 1 ("Es fraude"). Esta situación puede llevar a que los modelos den demasiada importancia a la clase 0 y se olviden de la 1. Para solventar este problema, se deberán aplicar técnicas de balanceo.

Por último, graficaré la variable numérica que considero más relevantes dentro del dataset: `TransactionAmt` (la cantidad de dinero que se movió en la transacción), para intentar descubrir si existe algún patrón en esas cantidades que nos pueda ayudar a detectar si una transacción ha sido o no fraudulenta.

Para la correcta visualización del histograma, se ha tenido que emplear una escala logarítmica en ambos ejes. Los datos crudos presentan una gran cola de valores en la parte derecha, que unida a la desproporción existente en las clases, hacía que fuese imposible extraer información del gráfico: 

```{r, message=F, warning=F}
# Histograma con los datos originales:
ggplot(data.raw) + 
  geom_histogram(aes(x=TransactionAmt, fill=as.factor(isFraud))) +
  labs(title = "Original data", x = "Transaction Amount", y = "# Transactions", fill = "isFraud")

# Histograma con escala logrítmica en el eje X:
ggplot(data.raw) + 
  geom_histogram(aes(x=TransactionAmt, fill=as.factor(isFraud))) +
  labs(title = "Log scale", x = "Transaction Amount", y = "# Transactions", fill = "isFraud") +
  scale_x_log10() + scale_y_log10()
```

La mayoría de las transaccciones se concentran en torno a los 100$, aunque no parece que exista un patrón claro que diferencie a las fraudulentas de las que no lo son, más o menos siguen la misma proporción. Lo único que destaca es que, para cantidades pequeñas, la proporción de transacciones fraudulentas es similar a las no fraudulentas (no hay demasiado desbalanceo). Esto puede significar dos cosas: o es más frecuente que las transacciones fraudulentas impliquen movimiento de poco dinero, o simplemente es cosa de cómo se ha hecho el muestreo.

Si nos fijamos en el histograma de los datos originales, en la parte derecha puede verse un gran hueco sin ninguna barra (a esto se refería la _gran cola de valores en la derecha_). Ese hueco representa que hay muy pocas transacciones en las que la cantidad de dinero que se transfirió fue muy elevada. Esto puede llevarnos a pensar que muchas de esas transacciones son _outliers_ (valores atípicos). Para estar completamente seguros, se dibujará un diagrama de caja sobre la columna `TransactionAmt`:

```{r}
ggplot(data.raw, aes(y=TransactionAmt)) +
  geom_boxplot() + scale_y_log10()
```

Efectivamente, los datos presentan una serie de _outliers_: tanto transferencias de sumas muy elevadas, como transacciones de muy poco dinero. Por otra parte, la gran mayoría de transacciones se concentran en sumas de en torno a los 100$ (tal y como evidenciaba el histograma).

A continuación, exploraremos el segundo dataset: `train_identity.csv`:

```{r, message=F}
data.id.raw <- read_csv('ieee-fraud-detection/train_identity.csv')
as.data.frame(df_status(data.id.raw, F))
```

Más o menos sigue en la misma línea que el otro dataset:
  
- Prácticamente todas las columnas (salvo `id_01` e `id_12`) tienen valores perdidos. El menor porcentaje es de 2.25 y 7 columnas tienen más de un 96% de NAs.
- Nuevamente aparecen columnas categóricas con demasiados valores únicos, como `id_19` con 522.
- También se observan variables categóricas y no categóricas con solo ceros o unos, como `id_35` o `id_38`.

```{r}
dim(data.id.raw)
```

Este dataset incluye 40 características nuevas a las 393 ya existentes (si no contamos el id). Por otro lado, reduce el número de filas, pues solo 144.233 ejemplos tienen informacion de identidad. Si ambos conjuntos de datos se uniese, se eliminarían 446.307 observaciones, por lo que puede resultar útil para reducir la dimensionalidad.

---

# Pruebas realizadas

En este apartado se describen los distintos experimentos realizados. Cada prueba está constituida por las siguientes fases:

1. Preprocesamiento de los datos: esta es la única fase que varía de un experimento a otro. Cada prueba aplica un preprocesamiento distinto al conjunto.
2. Entrenamiento de los modelos descritos en la [sección 2](#modelos-empleados). Para que la comparación de preprocesamientos (el principal objetivo de la práctica) sea justa, todos los modelos se entrenarán bajo las mismas condiciones.
3. Validación de los modelos. Se evaluan y comparan los modelos entrenados en la fase anterior calculando la curva ROC y el área bajo la curva ROC. Estas métricas serán las que se utilicen para elegir el mejor preprocesamiento de todos los realizados y también para elegir el mejor modelo entrenado.

> Nota: el código que realiza el preprocesamiento y entrenamiento de los modelos no se ha incluido en esta memoria para mantener su simplicidad y facilitar su lectura. En cada apartado se indica cuáles han sido los scripts ejecutados. Si desea consultar su contenido, puede encontrarlos bajo el directorio `scripts/`. Incluyen comentarios explicativos.

## Experimento 1

A continuación se describe brevemente el preprocesamiento realizado en este primer experimento, en el que solo se ha utilizado el dataset `train_transaction.csv` (no se han incluido las columnas del dataset `identity`):

- Borrar columnas con más de un 5% de valores perdidos. Asumiendo que el dataset incluye 590.540 ejemplos, que una columna presente tan solo un 15% de valores perdidos, implica que 88.581 muestras no tienen valor para ese campo. Son demasiados valores como para poder imputarlos. Por este motivo, todas aquellas columnas con más de un 5% de valores perdidos son descartadas. Es una condición bastante restrictiva, pero teniendo en cuenta que hay que reducir la dimensionalidad del dataset (y que la capacidad computacional de mi máquina no es muy elevada) es la única manera de hacer el conjunto más manejable, aunque por supuesto, seguramente estemos perdiendo información relevante.
- Borrar columnas con más de un 70% de 0. La justificación es la misma que en el paso anterior. Sacrificamos información a cambio de hacer el dataset abarcable para nuestra capacidad computacional. El procentaje se ha escogido teniendo en cuenta la información extraída en el análisis exploratorio (tabla `df_status`).
- Eliminar todos los ejemplos de la clase 0 con valores perdidos. De esta clase se dispone de un gran número de muestras, por lo que si eliminamos todas aquellas con valores perdidos, aún continuamos disponiendo de un número bastante elevado de ellas. De la clase minoritaria no podemos permitirnos perder ejemplos, pero de la clase 0 sí.
- Muestrear aleatoriamente tantos ejemplos de la clase 0 como ejemplos hay de la clase 1 (downsample). Con esto conseguimos balancear perfectamente el dataset.
- Imputar valores perdidos para la clase 1:
  - Para las variables categóricas se emplea la imputación KNN.
  - Las variable numéricas, son imputadas con la media de cada columna. Posiblemente este no sea el método más sofisticado, pero ha sido el único abarcable computacionalmente. Previamente a este decisión se intentó utilizar le paquete `MICE` para la imputación, pero no llegué a obtener ningún resultado (el ordenador se quedaba colgado por demasiada carga de trabajo). 
- Calcular la correlación de cada variable con la variable objetivo (`isFraud`) y eliminar todas aquellas con una correlación inferior a 0.05. Inicialmente el valor mínimo se fijó a 0.01, pero solo se eliminaba una columna, así que se aumentó para hacerlo más restrictivo.

Estos son los scripts ejecutados para preprocesar. Tras cada paso se muestra con un comentario como varía el tamaño del dataset:

```{r}
data <- data.raw
# DIM(data) = 590.540 x 394
args <- function() list(5)  # Argumentos para el script de abajo
source('scripts/clean_na.R')
# DIM(data) = 590.540 x 112
args <- function() list(70)
source('scripts/clean_zeros.R')
# DIM(data) = 590.540 x 52
args <- function() list(0)
source('scripts/drop_rows_na_class.R')
# DIM(data) = 578.728 x 52
source('scripts/random_downsample.R')
# DIM(data) = 41.326 x 52
```

Antes de imputar, convertimos las variables categóricas en factors (para que el KNN impute las columnas correctamente). También eliminamos la columna `TransactionID` pues no nos proporciona información a la hora de predecir si es fraude o no:

```{r}
data <- data %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate(card1 = as.factor(card1)) %>%
  mutate(card2 = as.factor(card2)) %>%
  mutate(card3 = as.factor(card3)) %>%
  mutate(card4 = as.factor(card4)) %>%
  mutate(card5 = as.factor(card5)) %>%
  mutate(card6 = as.factor(card6)) %>%
  select(-c(TransactionID))
```

```{r, warning=F}
args <- function() c("card2", "card3", "card4", "card5", "card6")
source('scripts/imputate.R')
source('scripts/objetive_correlations.R')
# DIM(data) = 41.326 x 31
```

### Entrenar

Una vez que se tiene el conjunto de datos preprocesado, el siguiente paso es entrenar ambos modelos (con los hiperparámetros escogidos y explicados en la [sección 2](#modelos-empleados)). Para entrenar se utilizará el 75% de los datos, guardando el 35% restante para validar los modelos:

```{r}
set.seed(89)
trainIndex <- createDataPartition(data$isFraud, p = .75, list = F, times = 1)
train <- data[trainIndex, ]
val <- data[-trainIndex, ]
```

**Random Forest:**

```{r}
# source('scripts/train_rf.R')
(rf <- readRDS('models/rf_v1.rds'))
```

Se obtiene un Accuracy en train de 0.766.

---

**Extreme Gradient Boosting:**

```{r}
# source('scripts/train_xgb.R')
(xgb <- readRDS('models/xgb_v1.rds'))
```

Se obtiene un Accuracy en train de 0.782.

### Validar

Por último, para comparar ambos modelos y poder valorar la bondad del preprocesamiento realizado, se dibujarán las curvas ROC y se calcularán las áreas bajo la curva ROC sobre el conjunto de validación previamente reservado. Puesto que será una operación que se realizará varias veces a lo largo de esta memoria, se creará una función:

```{r}
validate <- function(model1, model2, valdata) {
  # Predecir:
  pred1 <- predict(model1, valdata, type = 'prob')
  pred2 <- predict(model2, valdata, type = 'prob')
  
  # Calcular curvas ROC:
  auc1 <- roc(valdata$isFraud, pred1[["Yes"]])
  auc2 <- roc(valdata$isFraud, pred2[["Yes"]])
  
  # Mostar los plots para comparar:
  plot.roc(auc1, type = "S", print.auc = T, print.auc.y = 0.4, col="tomato", xlim = c(1,0))
  plot.roc(auc2, add = T, type = "S", print.auc = T, print.auc.y = 0.3, col="slateblue")
  legend("bottomright", legend=c("rf", "xgb"), col=c("tomato", "slateblue"), lwd=2)
}
```

Incluimos también la función presente en este script, la cual permite mostrar la matriz de confusión y las métricas más relevantes:

```{r}
source('scripts/plot_metrics.R')
```

```{r, message=F}
validate(rf, xgb, val)
plot_metrics(rf, xgb, val)
```

De todas las métricas calculadas, se utilizarán _AUC_, _Sensitivity_ y _Specificity_ para discutir los resultados. También se le prestará atención (aunque en menor medida), al _coeficiente Kappa de Cohen_:

- El área bajo la curva roc es una forma de resumir numéricamente la curva ROC. Cuanto más cercano a 1 sea su valor, mejor desempeño tendrá el clasificador. Es la métrica que se emplea en Kaggle para clasificar los modelos de la competición, por lo que es muy relevante para el problema.
- Sensitivity: es equivalente al _Recall_, e intuitivamente, es la capacidad del clasificador de encontrar todos los ejemplos positivos, en este caso, las transacciones fraudulentas.
- Specificity: para un problema binario, es equivalente al cálculo del _Recall_ pero para la clase negativa. Intuitivamente es la capacidad del clasificador de encontrar todos los ejemplos negativos, en este caso, las transacciones no fraudulentas.
- Kappa: mide el nivel de concordancia entre dos anotadores en un problema de clasificación. Intuitivamente, representa cuán cerca las instancias clasificadas por el clasificador coinciden con las etiquetas reales. Es más útil que el accuracy, sobre todo, en problemas desbalanceados.

En este caso, puede verse que todas las métricas de `xgboost` son superiores a las de `Random Forest`, por lo que, para este primer experimento, el mejor clasificador es `Extreme Gradient Boosting`. Obtiene un AUC de 0.87 y, puesto que la _Sensitivity_ es mayor que la _Specificity_, podemos deducir que encuentra más fácilmente los fraudes (algo bastante positivo, pues es el principal objetivo de este problema: detectar transacciones fraudulentas). Por último, con respecto al Kappa, 0.59 es, según las referencias, una concordancia moderada.

Para tratarse del primer experimento y, teniendo en cuenta que el preprocesado aplicado no es demasiado sofisticado, un AUC de 0.87 es realtivamente bueno, aunque aún puede mejorarse.

## Experimento 2

Para este segundo experimento, se emplearán los dos conjuntos de datos disponibles: `transaction` e `identity`. Estos son los pasos realizados:

* Unir ambos ficheros de datos según la columna `TransactionID`. Puesto que no todas las transacciones tienen asociada información de identidad, se realizará un _inner join_. Este eliminará todas aquellas filas del archivo `transaction` que no estén presentes en el dataset `identity`. De esta forma, se evita introducir aún más valores perdidos, al mismo tiempo que se contribuye a reducir la dimensionalidad de los datos (al menos en lo que al número de observaciones respecta).
* Extraer una submuestra del conjunto obtenido tras realizar el _inner join_. Esta submuestra se crea para que el proceso que se aplica en el paso posterior sea abarcable computacionalmente. Para ello, se realiza lo siguiente:
  * Eliminar columnas con más de un 50% de valores perdidos. Como ya se ha justificado en el experimento anterior, es prácticamente imposible tratar con éxito columnas con tantos valores perdidos, por lo que directamente no se tienen en cuenta. En este caso, la restricción es un poco más leve (antes se eliminaban columnas con más de un 5% de valores perdidos). Esto se ha hecho así para no descartar tantas columnas, solo las más inútiles.
  * Eliminar todos los ejemplos con valores perdidos. De esta forma se obtiene una muestra pequeña (con valores originales, no imputados), al mismo tiempo que se hace frente a los valores perdidos.
  * Balancear la submuestra utilizando el `random downsample` ya empleado en el experimento anterior (elegir de forma aleatoria tantos ejemplos de la clase 0 como ejemplos hay de la clase 1). Esto ayuda también a submuestrear aún más el conjunto.
* Con el subconjunto, se extraen las columnas más relevantes utilizando el paquete `earth`. Dicho paquete utiliza un modelo de regresión lineal ajustado sobre los datos para estudiar la importancia relativa de cada columna. Devuelve una lista ordenada con las características más importantes.
* Del conjunto original (el que se obtiene tras el _inner join_, sin submuestrear), nos quedamos solo con las columnas determinadas por el modelo de `earth` (las más relevantes) y la variable objetivo.
* Se preprocesa el conjunto que contiene las características más relevantes. El principal objetivo es tratar los valores perdidos y balancear, por lo que se aplican los mismos pasos que se realizaron en el primer experimento:
  * Borrar todos los ejemplos de la clase 0 con valores perdidos.
  * Balancear con un `random downsample`.
  * Imputar los valores perdidos de los ejemplos de la clase 1.
  
Realizar el _inner join_ y extraer el subconjunto:

```{r}
data.full.raw <- inner_join(data.raw, data.id.raw, by = 'TransactionID')
# DIM(data) = 144.233 x 434
data <- data.full.raw
args <- function() list(50)
source('scripts/clean_na.R')
# DIM(data) = 144.233 x 305
data <- data %>% drop_na()
# DIM(data) = 21.984 x 305
source('scripts/random_downsample.R')
# DIM(data) = 2.940 x 305
```

Convertir las variables que fueron cargadas como _logical_  a _numeric_ y las variables categóricas a _factors_. De esta forma, cada columna tendrá el tipo de dato apropiado. También se elimina la columna del id:

```{r}
data <- data %>%
  mutate_if(is.logical, as.numeric) %>%
  mutate_at(vars(contains('card')), as.factor) %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate_at(vars(contains('addr')), as.factor) %>%
  mutate_at(vars(contains('emaildomain')), as.factor) %>%
  mutate_at(vars(contains('Device')), as.factor) %>%
  mutate_at(append(paste0("id_", 12:20)[-7], paste0("id_", 28:36)), as.factor) %>%
  select(-c(TransactionID))
```

Estudiar la importancia de las columnas utilizando `earth`. Puesto que en el interior utiliza un modelo de regresión lineal, es necesario convertir los _factors_ a números. Si no se realiza esta conversion, `earth` realizará el _one hot encoding_ de forma automática. En ese caso, devolverá como variables importantes columnas del tipo *ColumnaCategórica_valor*, es decir, valores relevantes de columnas categóricas. Esto no nos interesa. Queremos saber la importancia de la columna completa, no de sus valores. Por eso convertimos a numérico:

```{r}
marsModel <- earth(isFraud ~ ., data = mutate_if(data, is.factor, as.numeric))
evimp(marsModel)
```

Seleccionar, de los datos originales, las columnas anteriores:

```{r}
data <- data.full.raw %>% 
  select(c(isFraud, C1,C14, ProductCD,
    D8, V338, TransactionAmt,
    TransactionDT, V187,
    R_emaildomain, V207)) %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate(R_emaildomain = as.factor(R_emaildomain))
```

Tratar los valores perdidos y balancear:

```{r, warning = F}
# DIM(data) = 144.233 x 11
args <- function() list(0)
source('scripts/drop_rows_na_class.R')
# DIM(data) = 48.274 x 11
source('scripts/random_downsample.R')
# DIM(data) = 22.636 x 11
args <- function() c("R_emaildomain")
source('scripts/imputate.R')
```

```{r, include=F}
lev <- levels(data$ProductCD)
```

### Entrenar

Al igual que en el experimento anterior, se utilizarán el 75% de los datos del nuevo conjunto para entrenar y el 35% para validar: 

```{r}
set.seed(89)
trainIndex <- createDataPartition(data$isFraud, p = .75, list = F, times = 1)
train <- data[trainIndex, ]
val <- data[-trainIndex, ]
```

**Random Forest:**

```{r}
# source('scripts/train_rf.R')
(rf <- readRDS('models/rf_v2.rds'))
```

Se obtiene un Accuracy en train de 0.9556.

---

**Extreme Gradient Boosting:**

```{r}
# source('scripts/train_xgb.R')
(xgb <- readRDS('models/xgb_v2.rds'))
```

Se obtiene un Accuracy en train de 0.9574.

### Validar

Utilizamos las funciones definidas en el experimento anterior para evaluar, con el conjunto de validación, los modelos entrenados:

```{r, message=F}
validate(rf, xgb, val)
plot_metrics(rf, xgb, val)
```

En este experimento, Random Forest supera ligeramente a xgboost (pese a que este último obtuvo un Accuracy un poco mayor en train). Todas sus metricas son levemente superiores, salvo _Specificity_. Esto significa que Random Forest encuentra mejor los fraudes y xgboost los no fraudes, aunque a grandes rasgos, Random Forest clasifica un poco mejor. Obtiene un Kappa de 0.916, que según las referencias, es casi perfecto.

En definitiva, los resultados son de lejos, muchísimo más satisfactorios que en el experimento anterior, puesto que casi se alcanza el 1 en el área bajo la curva ROC (0.992). Las columnas son muchas menos, pero más relevantes. Además, cabe destacar que, a excepción de `R_emaildomain` todas las columnas son del fichero `transaction`, por lo que puede deducirse que `identity` no aporta demasiada información relevante.

## Experimento 3

Para este último experimento, también se emplearán los dos ficheros disponibles: `transaction` e `identity`. A continuación se describen con detalle los pasos seguidos:

- Realizar el _inner join_ para unir ambos dataframes.
- Efectuar sobre el conjunto de datos un _análisis de componentes principales (PCA)_. Este es uno de los métodos más conocidos para reducir el número de características de un dataset (algo muy necesario en este conjunto de datos). Esta técnica ayuda a ver la "forma" general de los datos, identificando qué grupos de muestras son similares entre sí y calculando qué variables hacen que un grupo sea diferente de otro. Las _componentes principales_ no son más que direcciones en las que los datos están más esparcidos, es decir, donde se encuentra la mayor parte de la varianza. De esta manera, se transforma un conjunto de variables $X$ correlacionadas sobre muestras $Y$ en un conjunto de componentes principales $P$ no correlacionadas sobre las mismas muestras. Previamente a la aplicación de esta técnica, es necesario preparar los datos:
  - PCA solo trabaja con datos numéricos. Las variables categóricas deberían convertirse con _one hot encoding_ para que pudiesen ser procesadas por esta técnica. No obstante, estas columnas tienen cientos de valores diferentes y, teniendo en cuenta que _one hot encoding_ crea una columna por cada valor diferente, el dataset resultante tendría miles de columnas, algo inmanejable. Por tanto, para este experimento, solo se tendrán en cuenta las columnas numéricas. Esta decisión conlleva una pérdida de información considerable (concretamente se eliminan 46 columnas), pero es la única forma de poder aplicar esta técnica.
  - Eliminar columnas con más de un 50% de valores perdidos. La justificación es la misma que en el resto de experimentos: es prácticamente imposible tratar con éxito características con tantos valores perdidos.
  - Imputar los valores perdidos de las columnas restantes utilizando la mediana. En experimentos anteriores se ha utilizado la media. No obstante, para este se ha decidido utilizar la mediana puesto que, según referencias, funciona mejor cuando los datos tienen outliers (y en el análisis exploratorio se ha visto que los datos presentan algunos de ellos). 
  - Balancear el dataset mediante un _upsamplig_, es decir, generando ejemplos artificiales de la clase minoritaria. PCA transforma los datos, por lo que previamente a su aplicación se deben balancear. El downsampling provoca que el número de observaciones sea reducido (aproximadamente 22.000, como en el ejemplo anterior). Para evitar esto, en lugar de eliminar ejemplos de la clase mayoritaria, se crearan ejemplos nuevos de la clase minoritaria, balanceando sin perder tantas filas. Para generar los nuevos ejemplos, se ha utilizado `SMOTE`, una de las técnicas más conocidas y empleadas para este fin.
  - Eliminar outliers. Puesto que PCA trabaja con la varianza de los datos, eliminar los outliers detectados en el campo `TransactionAmt`, puede ayudar a mejorar los resultados (reduciendo la varianza).
  - Eliminar columnas con varianza 0. Estas impiden centrar y escalar los datos (restando la media y dividiendo entre la desviación estándar), un paso previo obligatorio para realizar correctamente el PCA.
- Una vez realizado el PCA, se debe escoger el número de componentes principales que se usarán finalmente para entrenar los modelos. Para ello, hay que buscar un equilibrio entre el número de componentes principales y el porcentaje de la varianza que explican. En este caso, con 20 componentes principales se explica el 80% de la varianza (es decir, el 80% de los datos) lo que parece una proporción razonable.
- Como 20 variables aún es demasiado, se intentará reducir aún más su número eliminando aquellas columnas que no estén demasiado correladas con la variable objetivo, de la misma forma que se hizo en el primer experimento (con un umbral de 0.05).

Partiendo de ambos datasets unidos, se convierte cada columna a su tipo de variable correcto. Todas numéricas salvo las columnas categóricas, que serán `factors`. De esta forma, se nos facilitarán los pasos posteriores:

```{r}
data.types <- data.full.raw %>%
  mutate(isFraud = as.factor(ifelse(isFraud == 1, 'Yes', 'No'))) %>%
  mutate_if(is.logical, as.numeric) %>%
  mutate_at(vars(contains('card')), as.factor) %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate_at(vars(contains('addr')), as.factor) %>%
  mutate_at(vars(contains('emaildomain')), as.factor) %>%
  mutate_at(paste0("M", 1:9), as.factor) %>%
  mutate_at(vars(contains('Device')), as.factor) %>%
  mutate_at(paste0("id_", 12:36), as.factor) %>%
  select(-c(TransactionID))
```

Se ejecuta todo el preprocesamiento asociado al PCA y el propio PCA:

```{r, message=F, warning=F}
source('scripts/pca.R' )
```

```{r, include=F}
keep.orig <- colnames(data.smoted)
keep.orig <- keep.orig[keep.orig!="isFraud"]
```

Estudiar los resultados del PCA. Cada columna representa una componente principal. La fila _Proportion of Variance_ muestra el porcentaje de la varianza explicado por esa componente y la fila _Cumulative Proportion_ el porcentaje explicado por esa componente junto a todas las demás. Como puede verse, hasta la PC20 se explica el 80.3% de los datos.

```{r}
as.data.frame(summary(data.pca)$importance)
```

Quedarnos con los datos transformados de las 20 primeras componentes y volver a añadir la variable objetivo:

```{r}
data <- as.data.frame(data.pca$x[, 1:20]) %>%
  mutate(isFraud = data.smoted$isFraud)
# DIM(data) = 42.359 x 21
```

Comprobar el balanceo: no es 50-50 pero sí muy similar.

```{r}
prop.table(table(data$isFraud))
```

Eliminar columnas cuya correlación con la variable objetivo es inferior a 0.05 (en valor absoluto):

```{r}
source('scripts/objetive_correlations.R')
# DIM(data) = 42.359 x 14
```

```{r, include=F}
keep.pca <- colnames(data)
keep.pca <- keep.pca[keep.pca!="isFraud"]
```


### Entrenar

Al igual que en el resto de experimentos, se utilizarán el 75% de los datos del nuevo conjunto para entrenar y el 35% para validar: 

```{r}
set.seed(89)
trainIndex <- createDataPartition(data$isFraud, p = .75, list = F, times = 1)
train <- data[trainIndex, ]
val <- data[-trainIndex, ]
```

**Random Forest:**

```{r}
# source('scripts/train_rf.R')
(rf <- readRDS('models/rf_v3.rds'))
```

Se obtiene un Accuracy en train de 0.8654.

---

**Extreme Gradient Boosting:**

```{r}
# source('scripts/train_xgb.R')
(xgb <- readRDS('models/xgb_v3.rds'))
```

Se obtiene un Accuracy en train de 0.8417.

### Validar

Se evalúan, con el conjunto de validación, los modelos entrenados:

```{r, message=F}
validate(rf, xgb, val)
plot_metrics(rf, xgb, val)
```

En este caso, Random Forest supera a xgboost en todas las métricas, sin excepciones. Tanto el _AUC_, como _Sensitivity_, _Specificity_ y _Kappa_ están por encima. Además, llama la atención que la _Sensitivity_ es mayor que la _Specificity_ en ambos modelos, lo que significa que los dos clasifican mejor los fraudes que los no fraudes. Por último, con respecto al Kappa, Random Forest obtiene 0.76, un valor considerado como "sustancial".

El AUC es significativamente mejor que en el primer experimento (0.94 frente a 0.87), pero ligeramente inferior que en el segundo (0.94 frente a 0.99). Aun así, las métricas evidencian que la transformación realizada por el PCA y la posterior selección de componentes, junto a la generación de ejemplos con SMOTE, han sido buenas decisiones. Los resultados son bastante satisfactorios, aunque no llegan a igualar al segundo experimento. Posiblemente, si no se hubiesen sacrificado las variables categóricas, los resultados serían mejores.

---

# Test

La siguiente tabla resume las métricas más relevantes obtenidas sobre el conjunto de validación en todos los experimentos:

|  Modelo 	|  AUC  	| Sensitivity 	| Specificity 	| Kappa 	|
|:-------:	|:-----:	|:-----------:	|:-----------:	|:-----:	|
|  RF(V1) 	|  0.84 	|     0.82    	|     0.71    	|  0.54 	|
| XGB(V1) 	|  0.87 	|     0.84    	|     0.74    	|  0.58 	|
|  RF(V2) 	| 0.992 	|     0.98    	|    0.933    	| 0.916 	|
| XGB(V2) 	|  0.99 	|     0.97    	|    0.937    	| 0.911 	|
|  RF(V3) 	|  0.94 	|     0.92    	|     0.85    	|  0.76 	|
| XGB(V3) 	|  0.92 	|     0.9     	|     0.8     	|  0.7  	|

Según la validación, el mejor modelo es **Random Forest del experimento 2**, seguido de **Random Forest del experimento 3**. Esta afirmación deriva de que RF(V2) es el que mayor AUC y coeficiente de Kappa presenta, seguido por RF(V3).

A continuación, se evaluará el mejor modelo (RF(V2)) sobre el cojunto de test. El primer paso es cargar ambos ficheros y unirlos por la columna `TransactionID`. En este caso no se emplea el _inner join_ porque no podemos eliminar filas. _left join_ colocará NA en aquellas transacciones que no tengan información de identidad asociada: 

```{r, message=F}
test.raw <- read_csv('ieee-fraud-detection/test_transaction.csv')
test.id.raw <- read.csv('ieee-fraud-detection/test_identity.csv')
test.full <- left_join(test.raw, test.id.raw, by = 'TransactionID')
# Para ahorrar memoria:
remove(test.raw, test.id.raw)
# Mostrar información sobre el conjunto de test:
as.data.frame(df_status(test.full))
```

Puede verse que presenta una gran cantidad de valores perdidos.

Renombramos algunas de las columnas de test que no tienen el mismo nombre que en train:

```{r}
colnames(test.full) <- gsub("id.", "id_", colnames(test.full))
```

Aplicamos el mismo preprocesamiento que se aplicó a los datos de train del experimento 2. 

Extraer las columnas indicadas por `earth`:

```{r}
data <- test.full %>% 
  select(c(C1, C14, ProductCD,
    D8, V338, TransactionAmt,
    TransactionDT, V187,
    R_emaildomain, V207)) %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate(R_emaildomain = as.factor(R_emaildomain))
```

Imputar los valores perdidos. Previamante a la imputación se modifican los _levels_ del _factor_ `ProductCD`. Esto se realiza porque en esta columna del test aparecen valores que no se tuvieron en cuenta en train, lo que supone un problema para Random Forest. La solución es eliminar esos valores (asignarle NA) y luego imputarlos. `lev` es una variable que contiene los _levels_ del _factor_ utilizado en train.

Por otro lado, la imputación que se realiza en test es diferente a la que se realiza en train. En test no es posible realizar la imputación de variables categóricas utilizando kNN (R no puede alojar un vector de 300 GB), por lo que en lugar de eso, los datos categóricos se imputarán de forma aleatoria pero siguiendo una determinada probabilidad. La probabilidad de cada valor es proprocional al porcentaje de veces que aparece en el propio conjunto. De esta forma, se mantiene la misma proporción en la muestra, aunque ubicada de forma aleatoria. Los datos numéricos si se imputan de la misma manera. Esto claramente afectará a la calidad de los resultados (pues el preprocesamiento no es exactamente el mismo en ambos conjunto) pero es la única forma de poder realizarlo. 

```{r}
data$ProductCD <- factor(data$ProductCD, levels = lev)
source('scripts/imputateV2.R')
```

Cargar el modelo y realizar la predicción:

```{r}
final.model <- readRDS('models/rf_v2.rds')
pred <- predict(final.model, data, type = 'prob')
```

Escribir el CSV que se enviará a Kaggle:

```{r}
results <- test.full %>%
  select(TransactionID) %>%
  mutate(isFraud = pred$Yes)

write_csv(results, 'res_rfv2.csv')
```

Kaggle nos devuelve un **AUC de 0.752754**. El resultado es claramente muchísimo más bajo que el que se obtuvo en train. Esto puede deberse a dos motivos:

1. La imputación de los valores perdidos en distinta en train y test. Esto afecta a la calidad, pero no hay otra solución.
2. El modelo entrenado presenta sobreajuste (aprende muy bien el conjunto de entrenamiento pero luego no es capaz de generalizar apropiadamente). El conjunto que se empleó para entrenar y validar es muchísimo más reducido que el que se emplea para testear (22.636 observaciones frente a 506.691). Al ser tan pocos datos en train, es bastante posible que el modelo los haya sobreaprendido, de ahí que de unas métricas tan excelentes (0.99 en AUC). La realidad es que luego no se comporta tan bien al ver datos nunca antes vistos.

A continuación, se probabrá el desempeño del segundo mejor modelo (RF(V3)) en el conjunto de test. Aplicamos el mismo preprocesamiento que se aplicó en train:

Convertir cada columna a su tipo de dato:

```{r}
data <- test.full %>%
  mutate_if(is.logical, as.numeric) %>%
  mutate_at(vars(contains('card')), as.factor) %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate_at(vars(contains('addr')), as.factor) %>%
  mutate_at(vars(contains('emaildomain')), as.factor) %>%
  mutate_at(paste0("M", 1:9), as.factor) %>%
  mutate_at(vars(contains('Device')), as.factor) %>%
  mutate_at(paste0("id_", 12:36), as.factor) %>%
  select(-c(TransactionID))
```

Escoger las mismas columnas que se mantuvieron en train (almacenadas en la variable `keep.orig`) e imputar. En este caso, no hay variables categóricas, por lo que la imputación es la misma en train y test. Un problema menos a la hora de evaluar el desempeño del modelo:

```{r}
data <- data %>%
  select(all_of(keep.orig))

source('scripts/imputateV2.R')
```

Transformamos los datos al espacio del PCA: 

```{r, message=F, warning=F}
data <- as.data.frame(predict(data.pca, newdata = data))
```

Por último, seleccionamos la mismas componentes, deshechando también las que se eliminaron tras estudiar la correlación con la variable objetivo (almacendas en `keep.pca`):

```{r}
data <- data %>%
  select(all_of(keep.pca))
```

Cargar el modelo, predecir y generar el CSV para Kaggle:

```{r}
final.model <- readRDS('models/rf_v3.rds')
pred <- predict(final.model, data, type = 'prob')

results <- test.full %>%
  select(TransactionID) %>%
  mutate(isFraud = pred$Yes)

write_csv(results, 'res_rfv3.csv')
```

Se obtiene un **AUC de 0.786960**. Como era de esperar se obtienen resultados mejores que con el anterior, pero tampoco es una mejora remarcable. Esto me hace pensar que el problema de los modelos no es el sobreajuste, si no los NA. En train se eliminaban aquellas columnas con más de un 50% de valores perdidos porque imputarlas no daría buenos resultados. En este caso, no ha sido posible eliminarlas y, como era de esperar, la imputación ha afectado negativamente a la calidad de los resultados.

---

# Conclusión

---

# Referencias

- [Reviewing IEEE-CIS Fraud Detection : Competition on Kaggle — XGBoost implementation — A Case Study](https://medium.com/@guildbilla/reviewing-ieee-cis-fraud-detection-competition-on-kaggle-top-2-solution-xgboost-b31e77b377b9)
- https://www.guru99.com/r-random-forest-tutorial.html
- https://towardsdatascience.com/understanding-random-forest-58381e0602d2
- https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/
- [GitHub jgromero - sige2020: lending_club_seleccion.Rmd](https://github.com/jgromero/sige2020/blob/master/Pr%C3%A1cticas/01%20Selecci%C3%B3n%20de%20variables/lending_club_seleccion.Rmd)
- http://r-statistics.co/Variable-Selection-and-Importance-With-R.html
- https://www.datacamp.com/community/tutorials/pca-analysis-r
- [STHDA - Principal Component Analysis in R](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/)
- http://amunategui.github.io/smote/
- https://rpubs.com/Mentors_Ubiqum/removing_outliers
- [Stackexchange - CrossValidated: replacement by mean vs median](https://stats.stackexchange.com/questions/143700/which-is-better-replacement-by-mean-and-replacement-by-median)
- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html
- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html
- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html
- https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english





