---
title: "Sistemas Inteligentes para la Gestión en la Empresa"
subtitle: "Práctica 1: Preprocesamiento de datos y clasificación binaria"
author: "Álvaro Fernández García"
date: "Curso 2019-2020"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: simplex
    highlight: tango
    number_sections: true
    df_print: paged
---

---

Antes que nada, para el desarrollo de la práctica se utilizarán los siguientes paquetes de R:

- `tidyverse`: para el manejo de los dataframes.
- `funModeling` y `ggplot2`: para el análisis exploratorio de los datos.
- `VIM`: para la imputación de variables perdidas.
- `caret`: para entrenar y validar modelos de clasificación.
- `pROC`: para calcular la curva ROC en la validación de los modelos.

```{r results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(VIM)
library(funModeling)
library(ggplot2)
library(caret)
library(pROC)
library(earth)
```

# Introducción

El pricipal objetivo de esta práctica es la resolución de un problema de preprocesamiento y aprendizaje automático, al mismo tiempo que se estudia la importancia de realizar un preprocesamiento apropiado. Para ello, se utilizará el conjunto de datos [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/data) en el que se pretende predecir la probabilidad de que una determinada transacción online sea o no fraudulenta.

Los datos están divididos en dos archivos `transaction` e `identity` (los cuales pueden unirse mediante la columna `TransactionID`). A continuación se describe el significado de cada una de las columnas de ambos datasets:

- `isFraud`: variable objetivo: 1 es fraude, 0 no lo es.
- `TransactionDT`: timedelta a partir de una fecha-hora de referencia dada (no es una marca de tiempo).
- `TransactionAMT`: importe de pago de la transacción en USD.
- `ProductCD`: código de producto, el producto para cada transacción.
- `card1 - card6`: información de la tarjeta de pago, como el tipo de tarjeta, la categoría de la tarjeta, el banco emisor, el país, etc.
- `addr`: dirección
- `dist`: distancia
- `P_ y R_ emaildomain`: dominio de correo electrónico del comprador y del destinatario
- `C1-C14`: contadores, como cuántas direcciones se encuentran asociadas a la tarjeta de pago, etc. El significado real está enmascarado.
- `D1-D15`: tiempo de espera, como por ejemplo, los días transcurridos entre la transacción anterior, etc.
- `M1-M9`: coincidencias, en los nombres, en la tarjeta, la dirección, etc.
- `Vxxx`: Vesta extrajo características ricas, incluyendo la clasificación, el conteo y otras relaciones de entidades.
- Las variables del dataset `identity` denotan: información de conexión a la red (IP, ISP, Proxy, etc.) y firma digital (UA/navegador/os/versión, etc.) asociada a las transacciones.

Todas las variables son numéricas, a excepción de las siguientes que son categóricas:

- **Transaction:**
  - ProductCD
  - card1 - card6
  - addr1, addr2
  - P_emaildomain
  - R_emaildomain
  - M1 - M9

- **Identity:**
  - DeviceType
  - DeviceInfo
  - id_12 - id_36

# Modelos empleados

En este apartado se describen aquellos algoritmos de clasificación que se aplicarán para intentar predecir si una transacción bancaria es o no fraudulenta. Para que la comparación entre los preprocesamientos realizados sea justa, los modelos empleados en todas las pruebas son los mismos. También se mantienen los mismos hiperparámetros, los cuales han sido fijados con el conjunto datos preprocesado del [experimento 1](#experimento-1): 

1. `Random Forest`: se trata de un metaestimador que ajusta un número determinado de árboles de decisión a varias submuestras. Se ha escogido este modelo para este problema simplemente por experiencia. En el pasado he trabajado en varios problemas de clasificación sobre conjuntos de datos muy similares a los que se obtienen tras preprocesar el _IEEE Fraud Detection_ (aproximadamente 40.000 datos, decenas de columnas y datos numéricos y categóricos). En esos problemas, Random Forest fue el método que mejores resultados obtuvo. En general, los _ensembles_ de clasificadores son bastante robustos y Random Forest es el ejemplo clásico de ensemble. Los hiperparámetros fijados han sido dos: el número de árboles (600) y el número de características a elegir en cada muestra (10). Para escogerlos, se ha hecho validación cruzada de 5 folds sobre una submuestra del conjunto de entrenamiento, con 5.000 ejemplos aleatorios de cada clase (para hacer su elección mucho más eficiente).
2. `Extreme Gradient Boosting`: se trata de una implementación de `Gradient Boosting Decision Trees` diseñada teniendo en cuenta la velocidad y el rendimiento. Gradient Boosting es un enfoque en el que se añaden nuevos modelos (boosting) que predicen los residuos o errores de modelos anteriores y luego se suman para hacer la predicción final. Se llama Gradient Boosting porque utiliza el algoritmo del gradiente descendente para minimizar la pérdida al agregar nuevos modelos (que en este caso son árboles de decisión). Se ha escogido este modelo por su eficiencia y también porque está ampliamente utilizado en las competiciones de Kaggle con resultados excelentes, además de que puede manejar sin problemas variables categóricas e incluso valores perdidos. Su principal inconveniente es que presenta una gran cantidad de hiperparámetros, por lo que para escoger sus valores óptimos, se ha dejado que caret los elija de forma automática de entre los valores por defecto, utilizando un subjunto de 1.000 ejemplos aleatorios de cada clase. Consulte `scripts/train_xgb.R` para más información.

**Nota:** antes de investigar sobre el Extreme Gradient Boosting, se apostó por utilizar una `Regresión Logística` con penalización L2. Siempre que se hace frente a un nuevo problema de aprendizaje automático, hay que empezar por la clase de modelos más simples, esto es, los lineales. El motivo por el que finalmente se descartó la Regresión Logística es porque este modelo es exclusivo para datos numéricos, y realizar el `one hot encoding` de variables categóricas con más de 70 valores únicos (incluso agrupando categorías poco frecuentes bajo una misma etiqueta) introducía demasiadas columnas en el dataset con varianza cero (por lo que los datos no podían normalizarse). En el script `scripts/train_lr.R` se encuentra la prueba que se realizó para el [experimento 1](#experimento-1), en la cual solo se obtuvo alrededor de un 60% de Accuracy en train.

# Análisis exploratorio

Antes de empezar con los distintos experimentos debemos extraer información sobre los datos. Esto lo conseguimos gracias al *análisis exploratorio*. La información que se obtenga en este proceso, será de utilidad para definir el preprocesamiento que se realizará en el futuro.

Comenzaremos analizando en primer lugar el dataset _transaction_. Cargamos el CSV, y con la función *df_status* del paquete _funModeling_ extraemos un resumen del dataset. En él se obtiene información como el procentaje de valores perdidos en cada columna, el porcentaje de ceros, el tipo de las columnas, el número de valores únicos, etcétera. En definitiva, proporciona información muy relevante para comprender mejor los datos.

```{r, message=FALSE}
data.raw <- read_csv('ieee-fraud-detection/train_transaction.csv')
as.data.frame(df_status(data.raw, F))
```

Puede verse que muchas de las columnas tienen un alto porcentaje de valores perdidos (89%, 86%, e incluso 93%) y lo mismo sucede con los 0. Esto será algo a lo que nos trendremos que enfrentar en el preprocesamiento. También llama la atención la presencia de varibales categóricas con demasiados valores únicos, como por ejemplo `card1`, con 13.553 valores diferentes. O muchas columnas categóricas (como `M7`) que solo tienen como valores 0 o 1 (por lo que son interpretadas como lógicas).
  
A continuación, mostramos la dimensión del dataset:

```{r}
dim(data.raw)
```

Está formado por casi 600.000 transacciones, con 394 variables (y estas no son todas las disponibles). Esta alta dimensionalidad puede suponer un grave problema a la hora de preprocesar y entrenar los modelos, por lo que habrá que aplicar técnicas para intentar reducirla.

También es de vital importancia conocer el balanceo de las clases, es decir, cuantos ejemplos hay de la clase "Es fraude" y cuantos de la clase "No es fraude":

```{r}
table(data.raw$isFraud)
prop.table(table(data.raw$isFraud))
```

Podemos ver que el desbalanceo es bastante acusado: el 96.5% de los datos pertenecen a la clase 0 ("No es fraude") mientras que solo el 3.5% pertenecen a la clase 1 ("Es fraude"). Esta situación puede llevar a que los modelos den demasiada importancia a la clase 0 y se olviden de la 1. Para solventar este problema, se deberán aplicar técnicas de balanceo.

Por último, graficaré la variable numérica que considero más relevantes dentro del dataset: `TransactionAmt` (la cantidad de dinero que se movió en la transacción), para intentar descubrir si existe algún patrón en esas cantidades que nos pueda ayudar a detectar si una transacción ha sido o no fraudulenta.

Para la correcta visualización del histograma, se ha tenido que emplear una escala logarítmica en ambos ejes. Los datos crudos presentan una gran cola de valores en la parte derecha, que unida a la desproporción existente en las clases, hacía que fuese imposible extraer información del gráfico: 

```{r, message=F, warning=F}
ggplot(data.raw) + 
  geom_histogram(aes(x=TransactionAmt, fill=as.factor(isFraud))) +
  labs(x = "Transaction Amount", y = "# Transactions", fill = "isFraud") +
  scale_x_log10() + scale_y_log10()
```

La mayoría de las transaccciones se concentran en torno a los 100$, aunque no parece que exista un patrón claro que diferencie a las fraudulentas de las que no lo son, más o menos siguen la misma proporción. Lo único que destaca es que, para cantidades pequeñas, la proporción de transacciones fraudulentas es similar a las no fraudulentas (no hay demasiado desbalanceo). Esto puede significar dos cosas: o es más frecuente que las transacciones fraudulentas impliquen movimiento de poco dinero, o simplemente es cosa de cómo se ha hecho el muestreo.

A continuación, exploraremos el segundo dataset: `train_identity.csv`:

```{r, message=F}
data.id.raw <- read_csv('ieee-fraud-detection/train_identity.csv')
as.data.frame(df_status(data.id.raw, F))
```

Más o menos sigue en la misma línea que el otro dataset:
  
- Prácticamente todas las columnas (salvo `id_01` e `id_12`) tienen valores perdidos. El menor porcentaje es de 2.25 y 7 columnas tienen más de un 96% de NAs.
- Nuevamente aparecen columnas categóricas con demasiados valores únicos, como `id_19` con 522.
- También se observan variables categóricas y no categóricas con solo ceros o unos, como `id_35` o `id_38`.

```{r}
dim(data.id.raw)
```

Este dataset incluye 40 características nuevas a las 393 ya existentes (si no contamos el id). Por otro lado, reduce el número de filas, pues solo 144.233 ejemplos tienen informacion de identidad. Si ambos conjuntos de datos se uniese, se eliminarían 446.307 observaciones, por lo que puede resultar útil para reducir la dimensionalidad.

---

# Pruebas realizadas

En este apartado se describen los distintos experimentos realizados. Cada prueba está constituida por las siguientes fases:

1. Preprocesamiento de los datos: esta es la única fase que varía de un experimento a otro. Cada prueba aplica un preprocesamiento distinto al conjunto.
2. Entrenamiento de los modelos descritos en la [sección 2](#modelos-empleados). Para que la comparación de preprocesamientos (el principal objetivo de la práctica) sea justa, todos los modelos se entrenarán bajo las mismas condiciones.
3. Validación de los modelos. Se evaluan y comparan los modelos entrenados en la fase anterior calculando la curva ROC y el área bajo la curva ROC. Estas métricas serán las que se utilicen para elegir el mejor preprocesamiento de todos los realizados y también para elegir el mejor modelo entrenado.

> Nota: el código que realiza el preprocesamiento y entrenamiento de los modelos no se ha incluido en esta memoria para mantener su simplicidad y facilitar su lectura. En cada apartado se indica cuáles han sido los scripts ejecutados. Si desea consultar su contenido, puede encontrarlos bajo el directorio `scripts/`. Incluyen comentarios explicativos.

## Experimento 1

A continuación se describe brevemente el preprocesamiento realizado en este primer experimento, en el que solo se ha utilizado el dataset `train_transaction.csv` (no se han incluido las columnas del dataset `identity`):

- Borrar columnas con más de un 5% de valores perdidos. Asumiendo que el dataset incluye 590.540 ejemplos, que una columna presente tan solo un 15% de valores perdidos, implica que 88.581 muestras no tienen valor para ese campo. Son demasiados valores como para poder imputarlos. Por este motivo, todas aquellas columnas con más de un 5% de valores perdidos son descartadas. Es una condición bastante restrictiva, pero teniendo en cuenta que hay que reducir la dimensionalidad del dataset (y que la capacidad computacional de mi máquina no es muy elevada) es la única manera de hacer el conjunto más manejable, aunque por supuesto, seguramente estemos perdiendo información relevante.
- Borrar columnas con más de un 70% de 0. La justificación es la misma que en el paso anterior. Sacrificamos información a cambio de hacer el dataset abarcable para nuestra capacidad computacional. El procentaje se ha escogido teniendo en cuenta la información extraída en el análisis exploratorio (tabla `df_status`).
- Eliminar todos los ejemplos de la clase 0 con valores perdidos. De esta clase se dispone de un gran número de muestras, por lo que si eliminamos todas aquellas con valores perdidos, aún continuamos disponiendo de un número bastante elevado de ellas. De la clase minoritaria no podemos permitirnos perder ejemplos, pero de la clase 0 sí.
- Muestrear aleatoriamente tantos ejemplos de la clase 0 como ejemplos hay de la clase 1 (downsample). Con esto conseguimos balancear perfectamente el dataset.
- Imputar valores perdidos para la clase 1:
  - Para las variables categóricas se emplea la imputación KNN.
  - Las variable numéricas, son imputadas con la media de cada columna. Posiblemente este no sea el método más sofisticado, pero ha sido el único abarcable computacionalmente. Previamente a este decisión se intentó utilizar le paquete `MICE` para la imputación, pero no llegué a obtener ningún resultado (el ordenador se quedaba colgado por demasiada carga de trabajo). 
- Calcular la correlación de cada variable con la variable objetivo (`isFraud`) y eliminar todas aquellas con una correlación inferior a 0.05. Inicialmente el valor mínimo se fijó a 0.01, pero solo se eliminaba una columna, así que se aumentó para hacerlo más restrictivo.

Estos son los scripts ejecutados para preprocesar. Tras cada paso se muestra con un comentario como varía el tamaño del dataset:

```{r}
data <- data.raw
# DIM(data) = 590.540 x 394
args <- function() list(5)  # Argumentos para el script de abajo
source('scripts/clean_na.R')
# DIM(data) = 590.540 x 112
args <- function() list(70)
source('scripts/clean_zeros.R')
# DIM(data) = 590.540 x 52
args <- function() list(0)
source('scripts/drop_rows_na_class.R')
# DIM(data) = 578.728 x 52
source('scripts/random_downsample.R')
# DIM(data) = 41.326 x 52
```

Antes de imputar, convertimos las variables categóricas en factors (para que el KNN impute las columnas correctamente). También eliminamos la columna `TransactionID` pues no nos proporciona información a la hora de predecir si es fraude o no:

```{r}
data <- data %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate(card1 = as.factor(card1)) %>%
  mutate(card2 = as.factor(card2)) %>%
  mutate(card3 = as.factor(card3)) %>%
  mutate(card4 = as.factor(card4)) %>%
  mutate(card5 = as.factor(card5)) %>%
  mutate(card6 = as.factor(card6)) %>%
  select(-c(TransactionID))
```

```{r, warning=F}
args <- function() c("card2", "card3", "card4", "card5", "card6")
source('scripts/imputate.R')
source('scripts/objetive_correlations.R')
# DIM(data) = 41.326 x 31
```

### Entrenar

Una vez que se tiene el conjunto de datos preprocesado, el siguiente paso es entrenar ambos modelos (con los hiperparámetros escogidos y explicados en la [sección 2](#modelos-empleados)). Para entrenar se utilizará el 75% de los datos, guardando el 35% restante para validar los modelos:

```{r}
set.seed(89)
trainIndex <- createDataPartition(data$isFraud, p = .75, list = F, times = 1)
train <- data[trainIndex, ]
val <- data[-trainIndex, ]
```

**Random Forest:**

```{r}
# source('scripts/train_rf.R')
(rf <- readRDS('models/rf_v1.rds'))
```

Se obtiene un Accuracy en train de 0.766.

---

**Extreme Gradient Boosting:**

```{r}
# source('scripts/train_xgb.R')
(xgb <- readRDS('models/xgb_v1.rds'))
```

Se obtiene un Accuracy en train de 0.782.

### Validar

Por último, para comparar ambos modelos y poder valorar la bondad del preprocesamiento realizado, se dibujarán las curvas ROC y se calcularán las áreas bajo la curva ROC sobre el conjunto de validación previamente reservado. Puesto que será una operación que se realizará varias veces a lo largo de esta memoria, se creará una función:

```{r}
validate <- function(model1, model2, valdata) {
  # Predecir:
  pred1 <- predict(model1, valdata, type = 'prob')
  pred2 <- predict(model2, valdata, type = 'prob')
  
  # Calcular curvas ROC:
  auc1 <- roc(valdata$isFraud, pred1[["Yes"]])
  auc2 <- roc(valdata$isFraud, pred2[["Yes"]])
  
  # Mostar los plots para comparar:
  plot.roc(auc1, type = "S", print.auc = T, print.auc.y = 0.4, col="tomato", xlim = c(1,0))
  plot.roc(auc2, add = T, type = "S", print.auc = T, print.auc.y = 0.3, col="slateblue")
  legend("bottomright", legend=c("rf", "xgb"), col=c("tomato", "slateblue"), lwd=2)
}
```


```{r, message=F}
validate(rf, xgb, val)
```

En este caso, el Extreme Gradient Boosting obtiene mejores resultados. Su curva ROC está ligeramente por encima de la del Random Forest lo que se refleja en un AUC de 0.87 frente a 0.84. No está nada mal para ser el primer experimento, teniendo en cuenta la cantidad de información sacrificada.

## Experimento 2

```{r}
data.full.raw <- inner_join(data.raw, data.id.raw, by = 'TransactionID')
data <- data.full.raw
args <- function() list(50)
source('scripts/clean_na.R')
data <- data %>% drop_na()
source('scripts/random_downsample.R')
# DIM(data) = 2940 x 305
```


```{r}
data <- data %>%
  mutate_if(is.logical, as.numeric) %>%
  mutate_at(vars(contains('card')), as.factor) %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate_at(vars(contains('addr')), as.factor) %>%
  mutate_at(vars(contains('emaildomain')), as.factor) %>%
  mutate_at(vars(contains('Device')), as.factor) %>%
  mutate_at(append(paste0("id_", 12:20)[-7], paste0("id_", 28:36)), as.factor) %>%
  select(-c(TransactionID))
```


```{r}
marsModel <- earth(isFraud ~ ., data = mutate_if(data, is.factor, as.numeric))
evimp(marsModel)
```

```{r}
data <- data.full.raw %>% 
  select(c(isFraud, C1,C14, ProductCD,
    D8, V338, TransactionAmt,
    TransactionDT, V187,
    R_emaildomain, V207)) %>%
  mutate(ProductCD = as.factor(ProductCD)) %>%
  mutate(R_emaildomain = as.factor(R_emaildomain))
```


```{r, warning = F}
args <- function() list(0)
source('scripts/drop_rows_na_class.R')
# DIM(data) = 48.274 x 11
source('scripts/random_downsample.R')
args <- function() c("R_emaildomain")
# DIM(data) = 22.636 x 11
source('scripts/imputate.R')
```

### Entrenar

```{r}
set.seed(89)
trainIndex <- createDataPartition(data$isFraud, p = .75, list = F, times = 1)
train <- data[trainIndex, ]
val <- data[-trainIndex, ]
```


---

# Referencias

- [Reviewing IEEE-CIS Fraud Detection : Competition on Kaggle — XGBoost implementation — A Case Study](https://medium.com/@guildbilla/reviewing-ieee-cis-fraud-detection-competition-on-kaggle-top-2-solution-xgboost-b31e77b377b9)
- https://www.guru99.com/r-random-forest-tutorial.html
- https://towardsdatascience.com/understanding-random-forest-58381e0602d2
- https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/
- [GitHub jgromero - sige2020: lending_club_seleccion.Rmd](https://github.com/jgromero/sige2020/blob/master/Pr%C3%A1cticas/01%20Selecci%C3%B3n%20de%20variables/lending_club_seleccion.Rmd)
- https://www.datacamp.com/community/tutorials/pca-analysis-r
- http://r-statistics.co/Variable-Selection-and-Importance-With-R.html




